# Comparative Analysis of 2D Adversarial Attacks in Autonomous Driving

This repository contains the experimental code for the research paper: **"Comparative Analysis of 2D Adversarial Attacks in Autonomous Driving: From Pixel-Level Integrity to System-Level Availability."**

## Overview

This research explores two distinct vectors of adversarial attacks on autonomous vehicle perception systems:

1.  **Pixel-Level Integrity (The "Lie"):** Using Projected Gradient Descent (PGD) to imperceptibly alter input images, causing misclassification in Deep Neural Networks (e.g., ResNet50).
2.  **System-Level Availability (The "Lag"):** Using the "SlowTrack" mechanism to flood the object detection pipeline (specifically Non-Maximum Suppression) with phantom bounding boxes, causing CPU spikes and dangerous latency.

## Repository Structure

- `src/pixel_level_pgd.py`: Script demonstrating PGD attacks on image classification.
- `src/system_level_latency.py`: Script demonstrating CPU resource exhaustion via NMS overload.

## Installation

1. Clone the repository.
2. Install the required dependencies:

```bash
pip install -r requirements.txt
```

*Note: It is recommended to install PyTorch strictly according to your hardware (CPU vs CUDA) from the [official website](https://pytorch.org/) before running the requirements file.*

## Usage

### 1\. Pixel-Level Integrity Experiment (The "Lie")

This module executes a Projected Gradient Descent (PGD) attack on a ResNet50 model. It demonstrates how imperceptible noise can force the AI to misclassify an object with high confidence.

```bash
python src/pixel_level_pgd.py
```

**Output:**

  * Downloads a test image (Standard OID/Coco sample).
  * Generates an adversarial perturbation.
  * Saves a visualization as `result_pixel_integrity.png`.

### 2\. System-Level Availability Experiment (The "Lag")

This module simulates a "SlowTrack" attack by flooding the perception pipeline with phantom objects. It targets the Non-Maximum Suppression (NMS) algorithm, creating a CPU bottleneck.

```bash
python src/system_level_latency.py
```

**Output:**

  * Runs a baseline latency test (Normal Traffic).
  * Runs a stress test (Attack Traffic).
  * Calculates the slowdown factor.
  * Saves a comparative bar chart as `result_system_availability.png`.

## Technical Methodology

### Attack Vector A: PGD (Projected Gradient Descent)

Unlike single-step attacks (like FGSM), PGD is an iterative method. It applies small perturbations ($\epsilon$) step-by-step to maximize the model's loss function while keeping the image visually unchanged to the human eye.

  * **Target:** Classification Integrity (Deep Neural Network).
  * **Constraint:** The noise is capped at $\epsilon = 8/255$ to remain "invisible" to the human driver.

### Attack Vector B: SlowTrack (NMS Exhaustion)

Object detection models output thousands of overlapping boxes. Non-Maximum Suppression (NMS) filters these down to the best candidates. This algorithm has a worst-case computational complexity of $O(N^2)$.

  * **Mechanism:** The attack generates 50,000+ adversarial "phantom" boxes clustered together.
  * **Result:** This forces the NMS algorithm into its worst-case execution path, spiking CPU usage and delaying the vehicle's reaction time (Availability Denial).

## Experimental Results

Below is a summary of the findings generated by this repository:

| Metric | Normal Conditions | Under Attack | Impact Analysis |
| :--- | :--- | :--- | :--- |
| **Prediction Class** | `Samoyed` (Correct) | `Paper Towel` (Incorrect) | **Critical Safety Failure** (Model Blindness) |
| **Confidence** | 92.5% | 99.1% | Model is "confident" in the wrong decision. |
| **Processing Time** | \~4 ms | \~250 ms | **60x Latency Spike** |
| **Frame Rate (FPS)** | \~240 FPS | \~4 FPS | System becomes unresponsive in real-time. |

## Future Scope

  * **3D Point Cloud Attacks:** Extending the PGD logic to LiDAR data processing.
  * **Adversarial Training:** Implementing defense mechanisms to make the model robust against $\epsilon$-perturbations.
  * **YOLO Integration:** Testing the SlowTrack attack on real-time YOLOv8 pipelines to benchmark edge-device performance.

## License

This project is for educational and research purposes.

```
```
